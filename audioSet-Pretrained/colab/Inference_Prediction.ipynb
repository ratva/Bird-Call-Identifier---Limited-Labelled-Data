{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/YuanGongND/ast/blob/master/colab/AST_Inference_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAW_eOOgiHN9"
   },
   "source": [
    "# [AST: Audio Spectrogram Transformer](https://www.isca-speech.org/archive/interspeech_2021/gong21b_interspeech.html)\n",
    "\n",
    "This colab script contains the implementation of a minimal demo of pretrained Audio Spectrogram Transformer (AST) inference and attention visualization.\n",
    "\n",
    "This script is self-contained and can be run in one click, replace the `sample_audio_path` to test your own audio.\n",
    "\n",
    "Please cite our paper if you find this repository useful.\n",
    "\n",
    "```\n",
    "@inproceedings{gong21b_interspeech,\n",
    "  author={Yuan Gong and Yu-An Chung and James Glass},\n",
    "  title={{AST: Audio Spectrogram Transformer}},\n",
    "  year=2021,\n",
    "  booktitle={Proc. Interspeech 2021},\n",
    "  pages={571--575},\n",
    "  doi={10.21437/Interspeech.2021-698}\n",
    "}\n",
    "```\n",
    "For more information, please check https://github.com/YuanGongND/ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7vplQ6tip8m"
   },
   "source": [
    "## Step 1. Install and import required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7DJZlND2Rbf4",
    "outputId": "4ad492d0-74c8-4b9e-a178-1fbc9a2d1640"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/avtar/miniconda3/envs/l3d_2024f_cpu/lib/python3.12/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/avtar/Library/CloudStorage/OneDrive-Tufts\n",
      "2005.62s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "Requirement already satisfied: timm==0.4.5 in /Users/avtar/miniconda3/envs/l3d_2024f_cpu/lib/python3.12/site-packages (0.4.5)\n",
      "Requirement already satisfied: torch>=1.4 in /Users/avtar/miniconda3/envs/l3d_2024f_cpu/lib/python3.12/site-packages (from timm==0.4.5) (2.4.1)\n",
      "Requirement already satisfied: torchvision in /Users/avtar/miniconda3/envs/l3d_2024f_cpu/lib/python3.12/site-packages (from timm==0.4.5) (0.19.1)\n",
      "Requirement already satisfied: filelock in /Users/avtar/miniconda3/envs/l3d_2024f_cpu/lib/python3.12/site-packages (from torch>=1.4->timm==0.4.5) (3.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/avtar/miniconda3/envs/l3d_2024f_cpu/lib/python3.12/site-packages (from torch>=1.4->timm==0.4.5) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/avtar/miniconda3/envs/l3d_2024f_cpu/lib/python3.12/site-packages (from torch>=1.4->timm==0.4.5) (1.13.1)\n",
      "Requirement already satisfied: networkx in /Users/avtar/miniconda3/envs/l3d_2024f_cpu/lib/python3.12/site-packages (from torch>=1.4->timm==0.4.5) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/avtar/miniconda3/envs/l3d_2024f_cpu/lib/python3.12/site-packages (from torch>=1.4->timm==0.4.5) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/avtar/miniconda3/envs/l3d_2024f_cpu/lib/python3.12/site-packages (from torch>=1.4->timm==0.4.5) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in /Users/avtar/miniconda3/envs/l3d_2024f_cpu/lib/python3.12/site-packages (from torch>=1.4->timm==0.4.5) (72.1.0)\n",
      "Requirement already satisfied: numpy in /Users/avtar/miniconda3/envs/l3d_2024f_cpu/lib/python3.12/site-packages (from torchvision->timm==0.4.5) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/avtar/miniconda3/envs/l3d_2024f_cpu/lib/python3.12/site-packages (from torchvision->timm==0.4.5) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/avtar/miniconda3/envs/l3d_2024f_cpu/lib/python3.12/site-packages (from jinja2->torch>=1.4->timm==0.4.5) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/avtar/miniconda3/envs/l3d_2024f_cpu/lib/python3.12/site-packages (from sympy->torch>=1.4->timm==0.4.5) (1.3.0)\n",
      "2011.46s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "Requirement already satisfied: wget in /Users/avtar/miniconda3/envs/l3d_2024f_cpu/lib/python3.12/site-packages (3.2)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    print('Running in Colab.')\n",
    "    !git clone https://github.com/YuanGongND/ast\n",
    "    sys.path.append('./ast')\n",
    "%cd ../\n",
    "\n",
    "! pip install timm==0.4.5\n",
    "! pip install wget\n",
    "import os, csv, argparse, wget\n",
    "os.environ['TORCH_HOME'] = '/Users/avtar/Library/CloudStorage/OneDrive-Tufts/Tufts CS/CS152 L3D/Project/Code/audioSet-Pretrained/pretrained_models'\n",
    "# if os.path.exists('/Users/avtar/Library/CloudStorage/OneDrive-Tufts/Tufts CS/CS152 L3D/Project/Code/audioSet-Pretrained/pretrained_models') == False:\n",
    "#   os.mkdir('/Users/avtar/Library/CloudStorage/OneDrive-Tufts/Tufts CS/CS152 L3D/Project/Code/audioSet-Pretrained/pretrained_models')\n",
    "\n",
    "sys.path.insert(0, \"/Users/avtar/Library/CloudStorage/OneDrive-Tufts/Tufts CS/CS152 L3D/Project/Code/audioSet-Pretrained/src/\")\n",
    "\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"dataloader\", \"/Users/avtar/Library/CloudStorage/OneDrive-Tufts/Tufts CS/CS152 L3D/Project/Code/audioSet-Pretrained/src/dataloader.py\")\n",
    "dataloader = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(dataloader)\n",
    "\n",
    "import torch, torchaudio, timm\n",
    "import numpy as np\n",
    "from torch.cuda.amp import autocast\n",
    "import IPython\n",
    "# import dataloader\n",
    "import models\n",
    "from utilities import *\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SW4NmX5wiv7f"
   },
   "source": [
    "## Step 2. Create AST model and load AudioSet pretrained weights.\n",
    "The pretrained model achieves 45.93 mAP on the AudioSet evaluation set, which is the best single model in the paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QiB9y5oKUQBV",
    "outputId": "6c5f1462-b8c9-4b93-89c1-eda66d4cc98b"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.models import ASTModel\n",
    "\n",
    "# Create a new class that inherits the original ASTModel class\n",
    "# class ASTModelVis(ASTModel):\n",
    "#     def get_att_map(self, block, x):\n",
    "#         qkv = block.attn.qkv\n",
    "#         num_heads = block.attn.num_heads\n",
    "#         scale = block.attn.scale\n",
    "#         B, N, C = x.shape\n",
    "#         qkv = qkv(x).reshape(B, N, 3, num_heads, C // num_heads).permute(2, 0, 3, 1, 4)\n",
    "#         q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "#         attn = (q @ k.transpose(-2, -1)) * scale\n",
    "#         attn = attn.softmax(dim=-1)\n",
    "#         return attn\n",
    "\n",
    "#     def forward_visualization(self, x):\n",
    "#         # expect input x = (batch_size, time_frame_num, frequency_bins), e.g., (12, 1024, 128)\n",
    "#         x = x.unsqueeze(1)\n",
    "#         x = x.transpose(2, 3)\n",
    "\n",
    "#         B = x.shape[0]\n",
    "#         x = self.v.patch_embed(x)\n",
    "#         cls_tokens = self.v.cls_token.expand(B, -1, -1)\n",
    "#         dist_token = self.v.dist_token.expand(B, -1, -1)\n",
    "#         x = torch.cat((cls_tokens, dist_token, x), dim=1)\n",
    "#         x = x + self.v.pos_embed\n",
    "#         x = self.v.pos_drop(x)\n",
    "#         # save the attention map of each of 12 Transformer layer\n",
    "#         att_list = []\n",
    "#         for blk in self.v.blocks:\n",
    "#             cur_att = self.get_att_map(blk, x)\n",
    "#             att_list.append(cur_att)\n",
    "#             x = blk(x)\n",
    "#         return att_list\n",
    "\n",
    "def make_features(wav_name, mel_bins, target_length=1024):\n",
    "    waveform, sr = torchaudio.load(wav_name)\n",
    "    # assert sr == 16000, 'input audio sampling rate must be 16kHz'\n",
    "\n",
    "    fbank = torchaudio.compliance.kaldi.fbank(\n",
    "        waveform, htk_compat=True, sample_frequency=sr, use_energy=False,\n",
    "        window_type='hanning', num_mel_bins=mel_bins, dither=0.0, frame_shift=10)\n",
    "\n",
    "    n_frames = fbank.shape[0]\n",
    "\n",
    "    # print(f'[*INFO] {wav_name} has {n_frames} frames')\n",
    "    # print(f'[*INFO] {wav_name} has dimensions {fbank.shape} and type {fbank.dtype}')\n",
    "    \n",
    "    # import matplotlib.pyplot as plt\n",
    "\n",
    "    # plt.imshow(fbank.T, aspect='auto', origin='lower')\n",
    "    # plt.title('Mel Spectrogram')\n",
    "    # plt.xlabel('Time')\n",
    "    # plt.ylabel('Mel Frequency')\n",
    "    # plt.colorbar(format='%+2.0f dB')\n",
    "    # plt.show()\n",
    "    \n",
    "    # print(f'fbank min: {fbank.min()}, fbank max: {fbank.max()}')\n",
    "    \n",
    "    p = target_length - n_frames\n",
    "    if p > 0:\n",
    "        m = torch.nn.ZeroPad2d((0, 0, 0, p))\n",
    "        fbank = m(fbank)\n",
    "    elif p < 0:\n",
    "        fbank = fbank[0:target_length, :]\n",
    "\n",
    "    fbank = (fbank - (-4.2677393)) / (4.5689974 * 2)\n",
    "    return fbank\n",
    "\n",
    "\n",
    "def load_label(label_csv):\n",
    "    with open(label_csv, 'r') as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        lines = list(reader)\n",
    "    labels = []\n",
    "    ids = []  # Each label has a unique id such as \"/m/068hy\"\n",
    "    for i1 in range(1, len(lines)):\n",
    "        id = lines[i1][1]\n",
    "        label = lines[i1][2]\n",
    "        ids.append(id)\n",
    "        labels.append(label)\n",
    "    return labels\n",
    "\n",
    "# Create an AST model and download the AudioSet pretrained weights\n",
    "# audioset_mdl_url = 'https://www.dropbox.com/s/cv4knew8mvbrnvq/audioset_0.4593.pth?dl=1'\n",
    "# if os.path.exists('../pretrained_models/audio_mdl.pth') == False:\n",
    "#   wget.download(audioset_mdl_url, out='../pretrained_models/audio_mdl.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------AST Model Summary---------------\n",
      "ImageNet pretraining: True, AudioSet pretraining: False\n",
      "frequncey stride=10, time stride=10\n",
      "number of patches=1212\n",
      "[*INFO] load checkpoint: /Users/avtar/Library/CloudStorage/OneDrive-Tufts/Tufts CS/CS152 L3D/Project/Code/exp/birdclef_audio1/models/best_audio_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/md/s3kx95_13hb51505hrc358j00000gn/T/ipykernel_63313/3416961813.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "audio_model = models.ASTModel(label_dim=12, fstride=10, tstride=10, input_fdim=128,\n",
    "input_tdim=1024, imagenet_pretrain=True,\n",
    "audioset_pretrain=False, model_size='base384')\n",
    "\n",
    "# Assume each input spectrogram has 1024 time frames\n",
    "input_tdim = 1024\n",
    "checkpoint_path = '/Users/avtar/Library/CloudStorage/OneDrive-Tufts/Tufts CS/CS152 L3D/Project/Code/exp/birdclef_audio1/models/best_audio_model.pth'\n",
    "# # now load the visualization model\n",
    "# ast_mdl = ASTModelVis(label_dim=12, input_tdim=input_tdim, imagenet_pretrain=True, audioset_pretrain=False)\n",
    "print(f'[*INFO] load checkpoint: {checkpoint_path}')\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "# Remove 'module.' prefix from keys\n",
    "state_dict = checkpoint\n",
    "new_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:] if k.startswith('module.') else k  # remove 'module.' prefix\n",
    "    new_state_dict[name] = v\n",
    "\n",
    "audio_model.load_state_dict(new_state_dict)\n",
    "audio_model = audio_model.to(device)\n",
    "audio_model.eval()          \n",
    "\n",
    "# Load the AudioSet label set\n",
    "label_csv = '/Users/avtar/Library/CloudStorage/OneDrive-Tufts/Tufts CS/CS152 L3D/Project/Code/audioSet-Pretrained/egs/audioset/data/bird_class_labels_indices.csv'       # label and indices for audioset data\n",
    "labels = load_label(label_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(audio_model, val_loader, args, epoch):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    batch_time = AverageMeter()\n",
    "    if not isinstance(audio_model, nn.DataParallel):\n",
    "        audio_model = nn.DataParallel(audio_model)\n",
    "    audio_model = audio_model.to(device)\n",
    "    # switch to evaluate mode\n",
    "    audio_model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    A_predictions = []\n",
    "    A_targets = []\n",
    "    A_loss = []\n",
    "    with torch.no_grad():\n",
    "        for i, (audio_input, labels) in enumerate(val_loader):\n",
    "\n",
    "            print(audio_input.shape)\n",
    "            print(labels.shape)\n",
    "\n",
    "            print(type(audio_input))\n",
    "            print(type(labels))\n",
    "            \n",
    "            audio_input = audio_input.to(device)\n",
    "\n",
    "            # compute output\n",
    "            audio_output = audio_model(audio_input)\n",
    "            audio_output = torch.sigmoid(audio_output)\n",
    "            predictions = audio_output.to('cpu').detach()\n",
    "\n",
    "            A_predictions.append(predictions)\n",
    "            A_targets.append(labels)\n",
    "\n",
    "            # compute the loss\n",
    "            labels = labels.to(device)\n",
    "            if isinstance(args.loss_fn, torch.nn.CrossEntropyLoss):\n",
    "                loss = args.loss_fn(audio_output, torch.argmax(labels.long(), axis=1))\n",
    "            else:\n",
    "                loss = args.loss_fn(audio_output, labels)\n",
    "            A_loss.append(loss.to('cpu').detach())\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "        audio_output = torch.cat(A_predictions)\n",
    "        target = torch.cat(A_targets)\n",
    "        loss = np.mean(A_loss)\n",
    "        stats = calculate_stats(audio_output, target)\n",
    "\n",
    "        # save the prediction here\n",
    "        exp_dir = args.exp_dir\n",
    "        if os.path.exists(exp_dir+'/predictions') == False:\n",
    "            os.mkdir(exp_dir+'/predictions')\n",
    "            np.savetxt(exp_dir+'/predictions/target.csv', target, delimiter=',')\n",
    "        np.savetxt(exp_dir+'/predictions/predictions_' + str(epoch) + '.csv', audio_output, delimiter=',')\n",
    "\n",
    "    return stats, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZg7FEmRj60I"
   },
   "source": [
    "## Step 3. Load an audio and predict the sound class.\n",
    "By default we test one sample from another dataset (VGGSound) that has not been seen during the model training.\n",
    "\n",
    "For this very specific sample, it is a parrot mimicing human speech and there's a bird chirp in the middle, that's why the model predicts it is a mixture of speech and bird sound. This audio is extracted from a Youtube video, and the video looks like this (note: only audio is used for prediction): \n",
    "\n",
    "![LDoXsip0BEQ_000177.jpeg](data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAASABIAAD/4QCMRXhpZgAATU0AKgAAAAgABQESAAMAAAABAAEAAAEaAAUAAAABAAAASgEbAAUAAAABAAAAUgEoAAMAAAABAAIAAIdpAAQAAAABAAAAWgAAAAAAAABIAAAAAQAAAEgAAAABAAOgAQADAAAAAQABAACgAgAEAAAAAQAAAUCgAwAEAAAAAQAAALMAAAAA/+0AOFBob3Rvc2hvcCAzLjAAOEJJTQQEAAAAAAAAOEJJTQQlAAAAAAAQ1B2M2Y8AsgTpgAmY7PhCfv/AABEIALMBQAMBIgACEQADEQD/xAAfAAABBQEBAQEBAQAAAAAAAAAAAQIDBAUGBwgJCgv/xAC1EAACAQMDAgQDBQUEBAAAAX0BAgMABBEFEiExQQYTUWEHInEUMoGRoQgjQrHBFVLR8CQzYnKCCQoWFxgZGiUmJygpKjQ1Njc4OTpDREVGR0hJSlNUVVZXWFlaY2RlZmdoaWpzdHV2d3h5eoOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4eLj5OXm5+jp6vHy8/T19vf4+fr/xAAfAQADAQEBAQEBAQEBAAAAAAAAAQIDBAUGBwgJCgv/xAC1EQACAQIEBAMEBwUEBAABAncAAQIDEQQFITEGEkFRB2FxEyIygQgUQpGhscEJIzNS8BVictEKFiQ04SXxFxgZGiYnKCkqNTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqCg4SFhoeIiYqSk5SVlpeYmZqio6Slpqeoqaqys7S1tre4ubrCw8TFxsfIycrS09TV1tfY2dri4+Tl5ufo6ery8/T19vf4+fr/2wBDAAgODhAOEBMTExMTExYVFhcXFxYWFhYXFxcZGRkdHR0ZGRkXFxkZHBwdHSAhIB4eHR4hISMjIyoqKCgxMTI8PEj/3QAEABT/2gAMAwEAAhEDEQA/AOfp9Npa1MxGNZ/8VXG6VS/ioA0k6/jVkdaxpbgW6bsbixwo6ZPXk9gBVy1nE6bgNpBwwznB+vcEdKL9CuV2vbQ6KMVor1qlEK0lHIoJNGOr1VY6uVIzPurNb2MRu7oocMdm35sZ+U5B4zzxzWhb2sNqpEa4J6seWP1Pp7dKnUVZqbCOV1pttsq8/NIOhwSQCQPzxXCWcW0lj8xQkJnPzO565P1Cg+ma9VurOO78rzM4jcvtH8XylcHvjnPHNcRcOUuS6bNqTYIxkdCuAAR91VzxnHpWM43MJK5IyxrNHGVeUwpvZFKYZ3OdzBmXONuQOmT7Vz2rX1y9vMrRSW6EbQMBjJn+867lVfbOT610SxM0jLOpUOFeO4VgcSnghWx8qkbdqtw2CD1rH1h3ispo5QA7ABWH3JPmGSvocclTyPcVlqmg1ujyiI/MPrUL8VNF94fWopOtbdT1vskA6H6GtCX7x/D+QqgO/wBDV6Tk/l/IUxIv2B/0y1/6+IP/AEYtfVZ618p2H/H3bf8AXxB/6NWvq09TWsNjmrbr0GYpKfRWxykdGKfSGgBlFOpKAGU2pKbQAykp9YWoXos0GAGd87QegA6sfz4Hek2lqzoo0Z16kacFeUtl+r8kbVeG67/x/TfVP/QFrsTqF/DteRTsfBUPGVVh/stx29ye+K4zU3W7u2dSsYk2cyNtVeADubBwAR1wazbutD1a2ArYRKcuWUXpzQd0n2ei1OIBxcof+mif+hrX1vXztJoVxH+/eayVMhtxuDt5Jxz5Xco2PpX0YqEgHI5A6n16URPJmPHSq/8AGP8AeP8AKraqcdup71EUO8dOpPX8K0Mkjkrv/UT/APXc/wAxVm0/49h9KLyJlt5iSMNLuGD7j2pLP/j1H41S2M3uf//QwaYxAGSQAOpPAp3asfUATD04DAt9Pf2BwTWuyISu0r21Fnu0iAx85bkBSMY9c9KSKVZgGGRzgg9QR2rlMAEkd637IFYi395yR9Bgf0rJSbZ11KSpxWt3cW8ztj9A5z7ZGBWKByeW5xwCQPrwetdeYvPjZM4JXg4zg7gQfzFU4tM+b95JuH91FK59ixJP5YpOLb0Kp1Yxi1JX1udfpjtJbRs/JO4ZPUgMQD+IrpVFZUChQqqAoAAAHAAHYVrrWpwt3bNBKtiqyVbFICwtTVEtCSxuzIrqzIcMoIJU4BwR1HBBpCJ68cvF+x3dwRnAlMo2nu2Hww6fN0B49DXsZIHJ6Dk/SvCbm4ieZ5ZeRK5L5DbQG4TPY4XAI/GsZ7Gc9juo5URAeGgkGcHkJu7/AO4f4h/D16Vh6+zR2RiOGXfGUZjzgOPl9yvr3UisuO5NnknBgJzt3ZaMnqVGMFT3XOfSq2sTxSWiLG6yJ5ilMH5ozg5XnnaRwB/CeOlYRvciF+ZI4KLqPrUUnWp4uufeopK16ns/ZKo7/Q1pydT+H8hWYO/0rVkHJ+g/kKbJiT2P/H1bf9d4f/Rq19Xnqa+UrL/j6tv+u8P/AKNWvq49a3hszlrbr0GUU6krY5RtFOpKAGUU6igBlNqSm0AMrmNS057sh0cBlQrtYHBz6EdD+BrqaSolFTVnsdWHxFTDVFUptKS7pNNPda9zjL7W0nsxbCAhtqq5k24QrgHYAScgjgnGK85YiC7tzL8gSWB33cbVEquSR2AXn6V7R9itfMMnkpvJ3ZIzz1zg8ZzzmvHfEX/H3cf7qf8AoIrBRlFatP0PcrYyjOiqNGnKEXLnm5O7cuy1en+Wx1aanicmW9szAuzaFvpGcbPN3My+XiTzN/KEgDC4zivU4yPLTnoqfouf5mvkG4J82QZOPm7/AOzX1pCcxRn1RP8A0EVrE8CeljSBHPTqf1BphbDjkdcd8884+vPemr0qFvvj/eH8quxkmZd84a3mUHJDJkc8Yxgfl1+tUbP/AI9fzp1z9y6/66L/AOgrTbP/AI9vzqlsRLdH/9HnuwpD+dHYUlbGZjSWELHKl4/ZSMfgCDj8Ku7VUAAYA4A9hVuoCOaVhtt9blqLv9B/M1dXrVOPgGrEToxwGU/Qg/1pks6KHoK11HNZkA4FbSjmkBbQVZxUaCrOKkCQdK8n1eH7PqEswB+dY3JUsrr8u3crKQ2Bt+YA9Oa9cA4rlNagLJHMP4CUb/dcjB/BgPwJrOV+V2Jlszkk1S6A8nzUkWVGAaU4KAjlxIOoAP3SMn1rVgW2hwVVrh8feCZA/wB0thFH0OfU15+52Sq0avsVyWwrAR8EFlJHTuQMiunF1cZ2gxPlNykqfmwcYyrY7jnFcjk7L9Tmu7I6Sa5kKEsI4kAyxc78AdcjhR+ZrgtVtnSyE7FgJJ1McZG3apUjcygABmAztAAXPTPNdBFC17cxiRy8aYlIxtQgEhVKjPzBh8wYnBX3qDxRITDHGOiuGc/7RVgq/llj6ACrjdG1O/MvU8uj7VHKKkj7fWmy1T3Pa6Io9j9K15Op+g/kKyD0P0rXmBH5L/6CKpmcdySzP+k2/wD13h/9GrX1iepr5Lsv+PiD/rvD/wCjVr62PWt4bHJW3XoMpKfSVsco2inUlADaSnUlACU2n0lADKSn0lADa8S8Qf8AH9N/up/6CK9vrxjXl3X0g9UT/wBBrOWxrDc8xuv9fL/wL/0Gvq625t4feKP/ANBFfIxmLs7OoYsD3K9vY+navriz5tYD/wBMY/8A0AUolT6GmvSoW+8P94fyqdelQv8Ae/4EtamKMC5Hy3f+8p/8dWorL/j2P1NWbkcXf/AP/QRVWw/49j9TTJe6P//S50dB9Kz2u4FOC+SP7oLfyBrMvJyT5SnAAG/Hckfd+g7+tYnoAPYAd/YAVXNrZG6pWg5yfou52SXEUvCuM+h4P5HBpzusYLNwAOf8+tccwwcMCCOcEEH6/wD1xUomkkAjJDBeRlgGPoOfvY6+tXLRXOOHvSs9C20jTFt3C5+5njp39T+lVgi9Rj6jt9CKmtwjvhs7RuJXBySoHy4xn3I9q1JkT5HRdoYlTxtzgZBxx6EVyqTTv+B6MowlaFumjOn0q7YssMhLZ+4x65H8LHvx0PXjBr0JeteMxZQgr1Uhl+oOR+texxOJFRx0dQw/EZrtaTSa2Z5EW05RlvF2NJKsVCtT1kbFhaSSNZEZHAZXUqwPcEYIpVqQ0AeO3ED2crRPuYgFkOOZU7Ediw6OOx56EVirHMkqyJHhBuzHuBb5upQDgeu3OPSvTtWUzPFC6psYB0Ykq6yBiMLIPu7gcdMHoetY1vpd5JLsBVlGMtICjKPXKgo/4bfpXHKGvu216HM42ehFYFYoZLjBxIxbpgkKAijB7sRx9a5nXkZbWHdy7zl3Pqxjbj6KMKPYV7vBpsEMaIRv2AcsO4749c81NJa2k2Y3jjfHVWUHGf8AGt1T212NYqzT7HyBGOnFNmGK+hb/AMLWrqWth5D9Qo5jPsV7fUYrwm+t5bVzHKu1h2/qPUUpRsepGakrGD2P0rcuP/ZV/wDQBWF2P0ramPT/AHV/9BFQxxI7Pi4h/wCu0P8A6MWvrqvkO0/4+If+usX/AKMWvr2uiGxyVd16DaKWitjlEpKdSUANpKfSUAMpKkptADKSnUlACV41r2f7QcAFmZUCqASWO3oAO9ezVRNrA0wnMSGUDAkI+YDGODUtXKTsfNE2j3ttCZJIHVcHnAOOO+0nH419M2PNpbf9cY//AEAVYI/HPX3qwoAUAAAAAADoBQlYbldE61A/X8Vqdahfr+K1RKMa5/5ev9xD+hqnYf8AHu31NXrn711/1zT+TVRsP9Q31NNEvdH/0/JpPvv/ALx/nVuzXMxP91Dj6k4z+X86qyfff/eP86RGaNgynacY6ZBB7EVMVe53Yh8sIduv3GleOSqqygNkFcNkgD73bpjj61gAj5s/X9OlWXYsSzHJPUn/ADwPYVbGnyOseT98/N0GxSM9e5xx9a0lF2scVKaUnLy0K0BzuOTkMCD3BxwR+GK2TNJLtD7QF5yCfmOMZwen05p00XI2AZAkB+m4Bf5GswEjOeMdc8Yrkle7R0utGMU3G8unY24zXf6XcjiBjg8mI+o6lPqOSPUfSuFtot43OMIBnb0L/X0X26n6VojqOoOcgjqCOQR7g9K9CleULNbbM+eqTcanNu5bo9hSp6yLGf7RCrH7wyr/AO8O/wBD1H1rXqNmegndJrqWFp59aatSCkUcnd3NrdQLLHIkiqDuXOG2N/snDcEA1b0u/VIGafIHmIqknJJPTJ46DkmuRitUh1AQvjy1d22EdgCykH0PGR61hz3Ecu5mYGNJMiIHG5gT97/ZA6+vSpja7diFq7n0OjpINyMGB7g5FG1Qd2AD3PrXnOg6n9rEyYICbSo46HI7cdRXbtJkVsURXE5HC8e5rkL/AE+HUoh5q8jow4YfQjn/ABrpGUMeajK46elAr2PmXUtJl09z1ePqHx09mxwD+lUZv4f9xf8A0EV9IXKLMrKVDA9QehHevF9Y0425DxqfL2jd/s84H4GsJROuE+jORtf9dF/11i/9GLX1+a+P7biaP/rrH/6MWvsGqhsZ1d16DaKWitjmEpKdSUAJRS0lACU2n0lAEdFOpKBDaSnUlADanHSoqlHSgZMtQydf++f51MtRSf8AxP8AOgaMm4+/cf8AXFf/AGas3TzmF/8AerVn/wBbP7wj+bVlad/qn/3qaJe6P//U8ok++3+8acuCKbJ99v8AeNRpjoXKn6Lg/Tiufc9qUlGCbTasuly4sQfOemMfnXQRSgoCxAK4VsnGCP8AEcisEBgOHYj22j+QqIIM5xk56nk/rVRvF66nj1KkZPRW/AtpPu3kAsWd8HooXccc/TnjPWnIu5974Zu3GAPoPX3PNV4TmMfj/M1ej60mtWzmlJvQ1CT8o/GrI61V/iX6VZBAIHc9Pw6n6CvYpfAjw6us2dppD4kkTsyhvxU4P6EflXbGvP8ASj/pS/8AXOX/ANkr0A1z1F7x6dB3pr5onXpWfd3sdkImkDFZJNhKjOzgncw67QcA46ZqhLq1jB8pmDsOCsQMjA++wED8TXOyX/8AaZnEcbIsELFSxG4ux44GQMbOOc88gVztpHc4SUeZppeZ1Wo2omj8+PG9EznPDoOcZ9QOVP4d64qK3tP9a9q8ztzvYRMP+AjeAPyz6064u7u3ASORRbyIxAKAlTjJQNkEBlJK8HGD7CrVzLIl2nkkImSkrYDAsQCDtPHy5UMRyd3tWDlzJWMZJ8sZK2t/wNiwMKyOI4TDuUDlVUHB4+6TXWA+tecyz3zeaj+UBEVIZFO91IySvICnGQM55FdFDLgJECXLDKueQy9QzZySQD+dbU5dCUzpMg/4VXOOccnvz29PYVAAehdm9cALn8QOBRLswFIO3rtXjd9fUfWugogLDIA5J/zjPt6Cs+aFJwyNhs8Nnp9Md6ts5JKxqAehx1A/kKQxuoHGfxx/KgDyW90NreUSwAlBIjFO4AcEke3sea+gutcU6yAZ24+hz/OtSC5kjADruX1B5FK1inJu1zoqMUkbrIMqciigkKSimqSS3sRj6FQaAsOopaKYhtJUuOM1jLqNk/3bqA/9tF/rQM06SohLGwJV0YAZO1gcAd+Cam680EjcUlPpKAGVKOlR1IKAJlqKTv8AQfzqVaik/oP50FIzp/8AWyf9cP6msbTv9W/1rdmH71/eH+prC037kn1pol7o/9XyiT77f7xqAgEYNW3GXbH94/zqv0rmPoI/CvRAhUfKyj2YD+YH86sqUHK7R9MVWCluACfp2/GrJjdELHYxHYjn8+hP4VV0eRWo2d42s+gW7AxKMgkDkf41qIKoQRyCOIYUBkDKSTz68AdR3q95ZVSWf6BRj9Tk/liiTSZwcj1fQub/AJ8D5mHRR/MnsPer6rtOScsep7fQew/XrWbCUiXcSEXqST1+pPU1HJcs5Pl5Rf7x+8f90Hp9Tz7V10661TVktjgWGqVpWgt929jrba+hspS75ZhGQsa8sSzL+CjjqSBWJe6nc3uQ7bIz/wAskJ2/8CPBf8cD2rHgieZ1iiUu7npnk+rMx7Dux6V6jaaBbIo+0Dz3PUZYRr7KoIzj+83P0rOUnUbeyPoqdOlg4pS9+R5rHg/eO1FHzYGAPbjoPU+legaKFZbgj7pZEXgDK7CeMdR83BPJrnnsXke48m3kWFXcAYIUhTjO5yMjj5QCfWu002F4POV9u4vG2FyQoMYAGeM8g84rls0zzq1WVWbbuktl2MmeJptO2j76xArn+/GP64wfY0t0EgSKPeWJt3wQMvI8hU7gAMkkjPA4rYhXAZT2llX8C5I/Q0ljbqkaSNl5CpTe2MqqsQEXjhRj6nqazj9peZyJ7rzM2KeSS5IkiaJmhibaxUtwzAkhSQOvrXJX189hfwLyUjVuAesbuG24HQrjGO4xXVTB7m7dIyYxGyq8oxnHl8xp/tEtknouB3rzzXEiiuYkiTaFQgtydzbhnLHlmH8RJ6mtY6T07GsEnM9pt5N8QKsH3fMcZ2kdep65NX2c5wQC3c9sjk49EU/ma8d0jVGtwIH+ZWYbCTwrEjAP+znn616hMxEb7TliQpY9+AcL7Zrvi7m84OLNeFBGo9SST7k9zU8hftjH0qJHAUE9+lXAQQO9WZGeucEt+VNVh0xxVncGOAaYRwcAGkBCSYTvQ49R2P1FXVuhIBjh89Ox/wA+lUSjbcH1qu6oFGM8daAOgbzvkK4/2lx1Jx3zxipU+8//AAH+WP6VkWtyM+W/J/hbJ59jz1raVSGY4wDtx+Gc0hskopaKZI5vut/un+Rr5cOSB9B/KvqU4xz9K5w6RpzdbaP8Bj+RFQ1ctHl3hwATXZwB/ordAB3r2hfur/uj+QrFXTrSyWZ4IyjNC4PzORtxnuSBzW4vQfQfyqkrIlhSU6kpkjaeKbThQBKtRydPw/rUi0yTp+B/nQUVJf8AW/WE/wAxXPad0k+tdHJ/rU94m/pXOaf1k+tNEvdH/9by0/6x/wDeNLWVMSJZOSPmPQn1qHew/ib8653E9dVNFp0OniPDD6VNjcrt/BGrEnsWAOAPp1PviuV8x/77fnUbO+zaXYgDGNxx+WcURik7nLUvK7Wh2P2m3jtIVc7m8tCFX7wO3rn+H8fpXOtcSy4y2AOy8fiT61TijeTARGb/AHVJH59PzNdNbaTJJzK4jHouGb8/uj9a2a5uhiuSG+vkZSEDBJJPbcSfyyf5V1Vppd3dkHb5Kf35AQT/ALqcMfx2iu0stPtrYgpGC2Pvv8z/AJnp9BXVLTUFfUuWIaVoJRILCwgsUxGCWbG+RuXfHr6D0UcCqkerJHdS210BAyP+7fJ8t0PKkk/dJ6ehIIB4roh0FU7qwtr4Dzk3FeFZSVcA9RuXBwfTpWhxpptuV3fr1H3l7bQQMZJkG4bVG4Ekk9gMnHcnoBya5+0vFuL2UKMK0SmPPVwjHLemCGGB19apnw0BL+4lWOM4zuTdIvrtYYDe24cHrmpru1GlSwzISIEIzu5OCNrqWPsd49SMdqxkmxVIwSXLJt9bq1jTHE04/wCmit/31Gn9QahjlMVmrLyzFggPQs8jbc+wzk+wpbl/KlnccgQxuMdz84GPrgYpix7DBEefIhUn3dhtB+oAb865Nrs4drlZQYVCJy5bCk/xSMeWOPfLN7CvPfEqLDc28S87IOT3JZySx92PJr0+2XfcM/aP92vu5++fwGFB+teSeIpfO1InHyiNFU/3gpbJ+hbIH0rSmupvRXvI5+Incv8AvD+Yr27T5WlRFfrw2SQc4XAwPrya8TjHzLn+8v8AMV61Z/ujbucEEDPsp4/HmuiDsz1Kiujv0TK47E4q8AEAAoWMPjPQZIx69M1SEjKxRxhh+oPQius80ucMRkVnXAI4GeTmru7J+lQ43MGHRaAIUZivOapM/wAxB545rWMgHNUZiCKAMxWAyp6N0PcfjW9aXZLCGU/N/A/98e/+1/OuZaORl3KPlB655qCZsRhgeU6HuKAPTKK5rTdQF0PLcjzAM/7w9fqO9dNSEJTSqnqo/KnUUAQMisMEcHqMnB/Wn0tFACU2nUlAhtOFJSigCVabJ0/A/wA6ctJJ0/A0DRWk/wBZH/1zb/2Wua0/70v1rqGHzxn/AGG/9lrmLDiSYe/9aa6ie6P/1+EexgckkMC3JIc9T7dKpnTYuzyD/vk/0roG7Uw1pYE33MT+y0/56v8Akv8AhT5LOCG3lYLuYRt8zHJHHbsPwFbxqldf8es3+4aLIm7fU0YgFjQDgbV/kK1Yayk+6v8Aur/IVqRUyGdTEOBWmOtZ8X3a0BS6iZfHQVaWqa9BVxaYFtKgu1DQ7SAQZIgQen+sFWUFYGrXawxrEp/eMyNnjEaq2d7kggA4wvB556A1L0QM5J2Ed4LeTO2NFbOCS0ayFolAAJJByD7Lz1qc3ZIlkKPG7uFjEiMBgAKjM2NoGcscnIFX44pj8yrEQ3O7zmJb3LeXzVsxXGMbIj7ea39Yq4Hd9PxOV3fT8SEr5Qjt4id7A/Px8q/xyn3JPy+rEds15F4iVU1AIowqQRKB6AbsV6TBILWaRBCzggH92yvtI42bmKhVxyq9jnivJ9YeWTUpWkQpkLtUlSVTaAMlSRnqTz3rWLOml8SM+P7yf7y/zFemqc2qkDAEYQY7nqT+deaRj5l7fMOfbNekAf6Ip5HyjaB3IB4PpVRPVn0O10bUFvIOTiSL5JB3yOh+jDmuplhEwB6OvQ+vsfavmK0v5dPvTIvTO2Rc8MvcfUdQf8a+jLG7iuI0ljbcrevUH0PuDXVF3R5848r8hjZDEHIPpUqPgYrRmj88tsxuXGfx6VkSxSRD5hj36itDAyrmQxnGauNtkjV1HYE1gXrqUJLAEVFa3LQqFcHaehINAzZjkCEqfutyPY1RuI8Zx91h/wDrFZk90oJCkHv7iqQ1EOpRuo6GgCWMyI6smVZOVbt+P17iu/03UlvQUcBJk+8meCP7y56j+VefW8quDg/Nnkf57VHLvjdZ4SFljOV9D6qfVW6EfjQM9ppKx9Ov49QgEqfKwO2RD96Nx1U/zB7jmtmkSNptOooAZRS0lACUopKWmIkXrSSdPwNOWh+n4H+VA0Qt96I/7Lf0rlrL/XzD3P8AOuqP/LL6H+VcxaDFxL9T/OhCfQ//0OYftUJqR+1Q1oIsmqd0f9Gm/wBw1aqjen/RZf8Ad/qKbIRqp91f90fyrUhrKj+4h/2V/kK1IaYHYQDK1cXpVODoKtjqfbmkSX4/61dFUo+h+tXV60AWmkEMTyN0RSfrjt+J4rx64uJZZS28fOWMjkfdwOQpzg7eF5GB9a9UvLZru3MKyeVuZCWwT8qsCVwCPvAYzniuPuLKC2dBuZhEAzZwEBP3VCKO3XByc4rmqJ28jGZj2lvcRrvWVrVPmY/xMQTkEq+UXj1BJzzzWkUuZZR5lxOYgmTGdiFiT8pby1UgEAnZnOMZ64rVG2JDPOCAvKR9TuPTIGd0hP3VH3frVeK2a5Bd7jeHZiywfKMn+Fn5kyowONvTpXOr23MknbcYXhg2hmVB0C9CfZVHJ/AV5zdWNzqmpymJGVAsYLyKyBRt9CMknsOvrivUpoksYJHgjRJMDDFclmJAG4k7m/OttF74GW5bHrinFcp00Y2d+x5sPC52jNz8wxx5fy/zzV64gNvCI3BBVPlcdGI6c/0r0kDiqF5b/aIJEx2JX2IHFWmeg5N7ny1OCJmz13c16HpWoGwe3DcxyQx7x6Hn5h7jv7VwF0CJzn15+vQ11Udhd3cdt5MRYeQnzEhVH3upJ/kDWquthuz3PpeAhvmBBDAHim3TBI2JGQATj1wK5fQLa9tICt08bAYEYRi5A/2mwB9AB+NX9VbzIJY1OCUJJB5A7fnXUcLVmY8UdpNIjHY7H5gN2ce+PaureOKVNjqpBGMYH6V4XoXF23ToOT7mvZlZDkgkgZ+6aZLOB1DSZISXiBdOvHVfr/jXDuueTx717q1wkKElsAcksR0/GuKvI7C8UsqtHIc4IAAc+46fjSbtuaRhKeyueYCaSJgcnI6Gt9LtZl64I/Ss+aB9nTOOh61ynzROd3B7MOlK6HKEouzTR3NnqB0+9SYH91IRHOvYgnAcehUnOfTNe/8AUZr5PklDghsA469m+o969z8O6nDd2cNuX/fwx7WVj8zBeN6k/eGMZPbvVXM2jvaSlopmY2kp1JQA2lFJSigCRetD9Pz/AJUq9aH6D8f5UDRF18r6H+Vcva/8fU31P866j+GL64/Rq5e34vJvqaEJ9D//0eUk7fWoe4+v9aillwXAXJTB+pbgfh61KR/q89cjP5E4rQRNVG9/49ZfoP5ilWbdsO37zFfXGOc9PTP41Wu33WchI284HOcgNgH8fSgVrG5EcxR/7i/yFasJ/nXPxyEJEOOWCcnnAT+EfX/GtVJCGCgZyrtn029Prk00Kx3UHStRcZrmoZyqn5CxWNGODwWfgKM/nmtb7QqGXcp/dhSenJbsPzpBys2F4q6lZ6Endn+9x7DaOPzrQjpk7GktectdJ5ss8vASRtof5EGGKh2ZuCTj5AoYgc4ya7O8u47KEyPkn7qIPvO56Kv8yegGSa8lmmlmm8+ZDK56bMFYgM4VFbHAyct95j7cVhUaSt1MZtL1DUZxfCIjz/kdnLbGjjwqk4QMQ5fOCHK9jjAOKvyRreqFZjHOACkqMyCYDn5thB5H3l6jOVrNjuVnddisVV2VmYbRuKkbQDyfc9KcWKARDgiRAjd1Vs7WAyMlSNuM/WuVtt9jBtu3RjEBZgxV4zHKFKu7ucgjPViApH3eMtkGvYgOK4uCCF5hL5483aAVMapyP4tj87scZz04rsUPAqkd1HZk4qQGogaXNUdR896nEtpqMwMaPh9wDjIw3PSpl1ueM5wuAPudFxjoB2rZ8VRFLpJR/wAtI8H6qf8A69eWs3Fapg1c9kg1WS8g3wyyxNHy8a7SSO+0EHIHbFclca9duojBjZHLHLLhmwf4zuGcj6elc3pt0LS5Rydq8hsDPGP8aXUxC0/mQ52SrvGeOc/McdgTzitbmfKe+6Lc2Wpwq4t4o5YgqMgA4BGQUPdD27g8GsTXtTeCQQ2oEZQ5eQAcn+4B0x/eJ+grwRHaM5VmU+qsVP5qQakM0p6ySHPq7H+ZNDkyVFXO8fUWvV8qfYGYjLcjPrjPHNWjJOEVGSYIGA35UkqO4Kk15yJHJGTmu9i86O2Z4mYMi5x2Ptg5FRzdGbxbi7xOlBib5dw5XKnOQfUHuDWNNaxTKQw2N2IrWk0y8kgiuYkhuVkjWQFF2SAMM4xnn8D+Fc6l4ikxzrJGRx8w+7+gOPqKUoNapnoU8RTqe7UitfuRyskLRHn5kPRh0rR068/s28huOqKSHA6lGGG/EcH8MV3rxRTRAKAy7fbpXB3GnuuWjO7GTt749R6kdxSjU7irYSy5oarsfUKOsiqynKsAwI7gjINPrwHR/EM1iqwTKZYV4XtJGPQZ4ZR2B5HrXulvPHcxJNGdyONynpx7jsfWutangtNFmkp1JTEMpRRRQIlXrSt0H1/pSL1pW6D60DIsZSP2P+NcvDxezfjXUj7ifX/GuXj4vpfx/lQhS6ep/9LlW60xulPbrTG6VqiBorO1D/j1f6p/6EK0hWdqH/Hq/wBU/wDQhSewI14P9TF/uL/IVqRdayoP9TF/uL/KtWLqKEB1kPQfQVqpznIB/CsuHpWonU0yS8Ohq9HVEdKvR0COG1h2a8CknCQoVHYF2bcfqdo/KuSmJ2xrkgPIFbHGVOeMjn8q6rV/+P5v+uEX/oUlcrN/yw/67L/I158/jOOfxkyALDwAMTnGOOk23+XFYnmOb6KMsSoYjBOeCjZGTyR9TW4v+p/7bn/0fXPf8xKP/f8A/ZHoXUa6/P8AI9PtP3sKb8Pyw+YA8AkDr7d66uLhV+lcpYf6lP8Aef8A9CNdZF91aiO5vh92WaXtSUdq0PRPNfFYHkW577mGfbBrxI17d4r/AOPe3/32/wDQTXiJraOwDB1qzOSXweigKo7AAdBVcdRU03+sb8P5CqJK1LSUtMB3avXbIAxEHvHz7/LXkXb869fsf9X/ANs//ZaykWj0Xw8SdKtc9kIH0DEAfgKl1iytri2keSJGZVJVsYYf8CGDUPh7/kFWv+63/obVr6j/AMec3+6f5V3LY4Hu/U8E05iAy5OAeB6V0pRSRx/e9ui5HSuX0/rJ9a6zuP8AgX/oNebP4j7DD/wV6HnuoKFljYDBZQWI7n1r2bwq7Pp7BiSFmcLnsDg4/M143qP34f8AcFev+E/+PCT/AK7t/IV20/hPnsX/ABH8j0akpaStjzRKQUtIKAJV605u31pq9ac3b60ARj7i/WuWT/j/AJP89q6kfcX61yyf8f8AJ/ntTQPof//Z)\n",
    "\n",
    "Replace the `sample_audio_path` to test your own audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 288
    },
    "id": "sapXfOwbhrzG",
    "outputId": "6120cb17-6889-403a-c2e2-0a0957d0025d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataloader.py file: /Users/avtar/Library/CloudStorage/OneDrive-Tufts/Tufts CS/CS152 L3D/Project/Code/audioSet-Pretrained/src/dataloader.py\n",
      "class_indices: /Users/avtar/Library/CloudStorage/OneDrive-Tufts/Tufts CS/CS152 L3D/Project/Code/audioSet-Pretrained/egs/audioset/data/bird_class_labels_indices.csv\n",
      "---------------the evaluation dataloader---------------\n",
      "now using following mask: 0 freq, 0 time\n",
      "now using mix-up with rate 0.000000\n",
      "now process imageNet\n",
      "use dataset mean -4.268 and std 4.569 to normalize the input.\n",
      "number of classes is 12\n"
     ]
    },
    {
     "ename": "LibsndfileError",
     "evalue": "Error opening 'Data/Audio/grewar3/XC189949.ogg': System error.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 97\u001b[0m\n\u001b[1;32m     70\u001b[0m te_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m     71\u001b[0m     dataloader\u001b[38;5;241m.\u001b[39mAudiosetDataset(te_data, label_csv\u001b[38;5;241m=\u001b[39mclass_indices, audio_conf\u001b[38;5;241m=\u001b[39mte_audio_conf),\n\u001b[1;32m     72\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# for i, (audio_input, labels) in enumerate(te_loader):\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m#     B = audio_input.size(0)\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m#     audio_input = audio_input.to(device, non_blocking=True)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m#     for k in range(10):\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m#         print('- {}: {:.4f}'.format(np.array(labels)[sorted_indexes[k]], result_output[sorted_indexes[k]]))\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m stats, _ \u001b[38;5;241m=\u001b[39m validate(audio_model, te_loader, args, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mListen to this sample: \u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[32], line 15\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(audio_model, val_loader, args, epoch)\u001b[0m\n\u001b[1;32m     13\u001b[0m A_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (audio_input, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(val_loader):\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28mprint\u001b[39m(audio_input\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28mprint\u001b[39m(labels\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda3/envs/l3d_2024f_cpu/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/l3d_2024f_cpu/lib/python3.12/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/l3d_2024f_cpu/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Tufts/Tufts CS/CS152 L3D/Project/Code/audioSet-Pretrained/src/dataloader.py:180\u001b[0m, in \u001b[0;36mAudiosetDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    178\u001b[0m datum \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[index]\n\u001b[1;32m    179\u001b[0m label_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_num)\n\u001b[0;32m--> 180\u001b[0m fbank, mix_lambda \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wav2fbank(datum[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwav\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label_str \u001b[38;5;129;01min\u001b[39;00m datum[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    182\u001b[0m     label_indices[\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_dict[label_str])] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Tufts/Tufts CS/CS152 L3D/Project/Code/audioSet-Pretrained/src/dataloader.py:101\u001b[0m, in \u001b[0;36mAudiosetDataset._wav2fbank\u001b[0;34m(self, filename, filename2)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wav2fbank\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename, filename2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# mixup\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename2 \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 101\u001b[0m         waveform, sr \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mload(filename)\n\u001b[1;32m    102\u001b[0m         waveform \u001b[38;5;241m=\u001b[39m waveform \u001b[38;5;241m-\u001b[39m waveform\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;66;03m# mixup\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/l3d_2024f_cpu/lib/python3.12/site-packages/torchaudio/_backend/utils.py:205\u001b[0m, in \u001b[0;36mget_load_func.<locals>.load\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03mBy default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m        `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    204\u001b[0m backend \u001b[38;5;241m=\u001b[39m dispatcher(uri, \u001b[38;5;28mformat\u001b[39m, backend)\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mload(uri, frame_offset, num_frames, normalize, channels_first, \u001b[38;5;28mformat\u001b[39m, buffer_size)\n",
      "File \u001b[0;32m~/miniconda3/envs/l3d_2024f_cpu/lib/python3.12/site-packages/torchaudio/_backend/soundfile.py:27\u001b[0m, in \u001b[0;36mSoundfileBackend.load\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     19\u001b[0m     uri: Union[BinaryIO, \u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     buffer_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4096\u001b[39m,\n\u001b[1;32m     26\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m soundfile_backend\u001b[38;5;241m.\u001b[39mload(uri, frame_offset, num_frames, normalize, channels_first, \u001b[38;5;28mformat\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/l3d_2024f_cpu/lib/python3.12/site-packages/torchaudio/_backend/soundfile_backend.py:221\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;129m@_requires_soundfile\u001b[39m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m    141\u001b[0m     filepath: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mformat\u001b[39m: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    148\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load audio data from file.\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m    Note:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m            `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m soundfile\u001b[38;5;241m.\u001b[39mSoundFile(filepath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file_:\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m file_\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWAV\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m normalize:\n\u001b[1;32m    223\u001b[0m             dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/l3d_2024f_cpu/lib/python3.12/site-packages/soundfile.py:658\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[1;32m    657\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[0;32m--> 658\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open(file, mode_int, closefd)\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n\u001b[1;32m    661\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/l3d_2024f_cpu/lib/python3.12/site-packages/soundfile.py:1216\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_ptr \u001b[38;5;241m==\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mNULL:\n\u001b[1;32m   1214\u001b[0m     \u001b[38;5;66;03m# get the actual error code\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m     err \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_error(file_ptr)\n\u001b[0;32m-> 1216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError opening \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode_int \u001b[38;5;241m==\u001b[39m _snd\u001b[38;5;241m.\u001b[39mSFM_WRITE:\n\u001b[1;32m   1218\u001b[0m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n\u001b[1;32m   1221\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mframes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mLibsndfileError\u001b[0m: Error opening 'Data/Audio/grewar3/XC189949.ogg': System error."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wget\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.cuda.amp import autocast\n",
    "# from dataloader import AudiosetDataset\n",
    "import IPython.display\n",
    "print(\"dataloader.py file: \" + dataloader.__file__);\n",
    "\n",
    "# Get a sample audio and make feature for predict\n",
    "# change url to play with the script\n",
    "#sample_audio_path = 'https://www.dropbox.com/s/kx8s8irzwj6nbeq/glLQrEijrKg_000300.flac?dl=1'\n",
    "\n",
    "# # some other samples\n",
    "sample_audio_path = 'https://www.dropbox.com/s/vddohcnb9ane9ag/LDoXsip0BEQ_000177.flac?dl=1'\n",
    "#sample_audio_path = 'https://www.dropbox.com/s/omned2muw8cyunf/6jiO0tPLK7U_000090.flac?dl=1'\n",
    "\n",
    "pathToCode = \"/Users/avtar/Library/CloudStorage/OneDrive-Tufts/Tufts CS/CS152 L3D/Project/Code/\"\n",
    "te_data = f\"{pathToCode}Data/test_audio.json\"\n",
    "\n",
    "class Args:\n",
    "    pass\n",
    "\n",
    "args = Args()\n",
    "args.alen = 5\n",
    "args.audio_length = 1024\n",
    "args.audioset_pretrain = False\n",
    "args.bal ='bal'\n",
    "args.batch_size =12\n",
    "args.data_eval ='/Users/avtar/Library/CloudStorage/OneDrive-Tufts/Tufts CS/CS152 L3D/Project/Code/Data/test_audio.json'\n",
    "args.data_train ='/Users/avtar/Library/CloudStorage/OneDrive-Tufts/Tufts CS/CS152 L3D/Project/Code/Data/train_audio.json'\n",
    "args.data_val ='/Users/avtar/Library/CloudStorage/OneDrive-Tufts/Tufts CS/CS152 L3D/Project/Code/Data/val_audio.json'\n",
    "args.dataset ='audioset'\n",
    "args.dataset_mean =-4.2677393\n",
    "args.dataset_std =4.5689974\n",
    "args.exp_dir ='./exp/birdclef_audio1x'\n",
    "args.freqm =48\n",
    "args.fstride =10\n",
    "args.imagenet_pretrain =True\n",
    "args.label_csv ='/Users/avtar/Library/CloudStorage/OneDrive-Tufts/Tufts CS/CS152 L3D/Project/Code/audioSet-Pretrained/egs/audioset/data/bird_class_labels_indices.csv'\n",
    "args.loss ='BCE'\n",
    "args.lr =1e-05\n",
    "args.lrscheduler_decay =0.5\n",
    "args.lrscheduler_start =2\n",
    "args.lrscheduler_step =1\n",
    "args.metrics ='mAP'\n",
    "args.mixup =0.5\n",
    "args.model ='ast'\n",
    "args.n_class =12\n",
    "args.n_epochs =5\n",
    "args.n_print_steps =100\n",
    "args.noise =False\n",
    "args.num_workers =0\n",
    "args.optim ='adam'\n",
    "args.save_model =True\n",
    "args.timem =192\n",
    "args.tstride =10\n",
    "args.wa =True\n",
    "args.wa_end =5\n",
    "args.wa_start =1\n",
    "args.warmup =True\n",
    "\n",
    "te_audio_conf = {'num_mel_bins': 128, 'target_length': 1024, 'freqm': 0, 'timem': 0, 'mixup': 0, 'dataset': 'imageNet', 'mode':'evaluation', 'mean':-4.2677393, 'std':4.5689974, 'noise':False}\n",
    "\n",
    "class_indices = f\"{pathToCode}audioSet-Pretrained/egs/audioset/data/bird_class_labels_indices.csv\"\n",
    "print(\"class_indices: \" + class_indices)\n",
    "\n",
    "input_tdim = 1024  # Define input_tdim\n",
    "\n",
    "te_loader = torch.utils.data.DataLoader(\n",
    "    dataloader.AudiosetDataset(te_data, label_csv=class_indices, audio_conf=te_audio_conf),\n",
    "    batch_size=12*2, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "# for i, (audio_input, labels) in enumerate(te_loader):\n",
    "#     B = audio_input.size(0)\n",
    "#     audio_input = audio_input.to(device, non_blocking=True)\n",
    "#     labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "#     feats = make_features('/Users/avtar/Library/CloudStorage/OneDrive-Tufts/Tufts CS/CS152 L3D/Project/Code/audioSet-Pretrained/sample_audios/XC37740.ogg', mel_bins=128) \n",
    "\n",
    "#     feats_data = feats.expand(1, input_tdim, 128)  # reshape the feature\n",
    "#     feats_data = feats_data.to(device)\n",
    "\n",
    "#     # Make the prediction\n",
    "#     with torch.no_grad():\n",
    "#         with autocast():\n",
    "#             output = audio_model.forward(feats_data)\n",
    "#             output = torch.sigmoid(output)\n",
    "#     result_output = output.data.cpu().numpy()[0]\n",
    "#     sorted_indexes = np.argsort(result_output)[::-1]\n",
    "\n",
    "#     # Print audio tagging top probabilities\n",
    "#     print('Predice results:')\n",
    "#     for k in range(10):\n",
    "#         print('- {}: {:.4f}'.format(np.array(labels)[sorted_indexes[k]], result_output[sorted_indexes[k]]))\n",
    "\n",
    "stats, _ = validate(audio_model, te_loader, args, 1)\n",
    "print('Listen to this sample: ')\n",
    "\n",
    "# # IPython.display.Audio('../sample_audios/sample_audio.flac')\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# from torch.cuda.amp import autocast\n",
    "# from dataloader import AudiosetDataset\n",
    "# from utils import make_features\n",
    "# import IPython.display\n",
    "\n",
    "# # Get a sample audio and make feature for predict\n",
    "# # change url to play with the script\n",
    "# #sample_audio_path = 'https://www.dropbox.com/s/kx8s8irzwj6nbeq/glLQrEijrKg_000300.flac?dl=1'\n",
    "\n",
    "# # # some other samples\n",
    "# sample_audio_path = 'https://www.dropbox.com/s/vddohcnb9ane9ag/LDoXsip0BEQ_000177.flac?dl=1'\n",
    "# #sample_audio_path = 'https://www.dropbox.com/s/omned2muw8cyunf/6jiO0tPLK7U_000090.flac?dl=1'\n",
    "\n",
    "# pathToCode = \"/Users/avtar/Library/CloudStorage/OneDrive-Tufts/Tufts CS/CS152 L3D/Project/Code/\"\n",
    "# te_data = f\"{pathToCode}Data/test_audio.json\"\n",
    "\n",
    "# te_audio_conf = {'num_mel_bins': 128, 'target_length': 1024, 'freqm': 0, 'timem': 0, 'mixup': 0, 'dataset': 'imageNet', 'mode':'evaluation', 'mean':-4.2677393, 'std':4.5689974, 'noise':False}\n",
    "\n",
    "# class_indices = f\"{pathToCode}audioSet-Pretrained/egs/audioset/data/bird_class_labels_indices.csv\"\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "\n",
    "# input_tdim = 1024  # Define input_tdim\n",
    "\n",
    "# te_loader = torch.utils.data.DataLoader(\n",
    "#     AudiosetDataset(te_data, label_csv=class_indices, audio_conf=te_audio_conf),\n",
    "#     batch_size=12*2, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# for i, (audio_input, labels) in enumerate(te_loader):\n",
    "#     B = audio_input.size(0)\n",
    "#     audio_input = audio_input.to(device, non_blocking=True)\n",
    "#     labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "#     feats = make_features('/Users/avtar/Library/CloudStorage/OneDrive-Tufts/Tufts CS/CS152 L3D/Project/Code/audioSet-Pretrained/sample_audios/XC37740.ogg', mel_bins=128) \n",
    "\n",
    "#     feats_data = feats.expand(1, input_tdim, 128)  # reshape the feature\n",
    "#     feats_data = feats_data.to(device)\n",
    "\n",
    "#     # Make the prediction\n",
    "#     with torch.no_grad():\n",
    "#         with autocast():\n",
    "#             output = audio_model.forward(feats_data)\n",
    "#             output = torch.sigmoid(output)\n",
    "#     result_output = output.data.cpu().numpy()[0]\n",
    "#     sorted_indexes = np.argsort(result_output)[::-1]\n",
    "\n",
    "#     # Print audio tagging top probabilities\n",
    "#     print('Predice results:')\n",
    "#     for k in range(10):\n",
    "#         print('- {}: {:.4f}'.format(np.array(labels)[sorted_indexes[k]], result_output[sorted_indexes[k]]))\n",
    "\n",
    "# print('Listen to this sample: ')\n",
    "# IPython.display.Audio('../sample_audios/sample_audio.flac')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPknIqIY2a2l"
   },
   "source": [
    "## Step 4.a. Visualize the mean attention map of all attention heads for each layer\n",
    "Note the model focuses on the middle high frequency bird chirp as well as the low frequency speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SWVFQGcN2bv3",
    "outputId": "b7c3ede8-891d-43ae-92a4-17fe229c4297"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(feats_data[0].t().cpu(), origin='lower')\n",
    "plt.title('Original Spectrogram')\n",
    "plt.show()\n",
    "plt.close()\n",
    "# Make the prediction\n",
    "with torch.no_grad():\n",
    "  with autocast():\n",
    "    att_list = audio_model.module.forward_visualization(feats_data)\n",
    "for i in range(len(att_list)):\n",
    "  att_list[i] = att_list[i].data.cpu().numpy()\n",
    "  att_list[i] = np.mean(att_list[i][0], axis=0)\n",
    "  att_list[i] = np.mean(att_list[i][0:2], axis=0)\n",
    "  att_list[i] = att_list[i][2:].reshape(12, 101)\n",
    "  plt.imshow(att_list[i], origin='lower')\n",
    "  plt.title('Mean Attention Map of Layer #{:d}'.format(i))\n",
    "  plt.show()\n",
    "  plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S10lS5hm2-Tc"
   },
   "source": [
    "## Step 4.b. Visualize the attention map of each attention head of a specific layer\n",
    "Note the attention map of heads are quite diverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JAJL6IqJ3IFM",
    "outputId": "88ecad5a-00bf-43d5-f2d1-2848547dc4f6"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(feats_data[0].t().cpu(), origin='lower')\n",
    "plt.title('Original Spectrogram')\n",
    "plt.show()\n",
    "plt.close()\n",
    "# Make the prediction\n",
    "with torch.no_grad():\n",
    "  with autocast():\n",
    "    att_list = audio_model.module.forward_visualization(feats_data)\n",
    "\n",
    "# the layer of interest\n",
    "i = 11\n",
    "\n",
    "att_map = att_list[i].data.cpu().numpy()\n",
    "att_map = att_map[0]\n",
    "att_map = np.mean(att_map[:, 0:2, :], axis=1)\n",
    "att_map = att_map[:, 2:].reshape(12, 12, 101)\n",
    "for j in range(12):\n",
    "  plt.imshow(att_map[j], origin='lower')\n",
    "  plt.title('Mean Attention Map of Head #{:d} Layer #{:d}'.format(j, i))\n",
    "  plt.show()\n",
    "  plt.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOU6ja/3lV62pswJ2YerBsB",
   "collapsed_sections": [],
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "l3d_2024f_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
